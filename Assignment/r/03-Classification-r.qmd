---
title: "Classification: LDA and QDA"
author: "YOUR NAME HERE"
format: 
  html:
    code-fold: true
    code-line-numbers: true
    code-tools: true
    self-contained: true
editor: visual
execute:
  message: false
---

## Setup

Declare your libraries:

```{r}
#| label: libraries-r
#| include: false
library(tidyverse)
library(tidymodels)
library(glmnet)
library(discrim)
```


# LDA

In this lab, we will use medical data to predict the likelihood of a person experiencing an exercise-induced heart attack.  

Our dataset consists of clinical data from patients who entered the hospital complaining of chest pain ("angina") during exercise.  The information collected includes:

* `age` : Age of the patient

* `sex` : Sex of the patient

* `cp` : Chest Pain type

    + Value 1: typical angina
    + Value 2: atypical angina
    + Value 3: non-anginal pain
    + Value 4: asymptomatic
    
* `trtbps` : resting blood pressure (in mm Hg)

* `chol` : cholesteral in mg/dl fetched via BMI sensor

* `restecg` : resting electrocardiographic results

    + Value 0: normal
    + Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)
    + Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria

* `thalach` : maximum heart rate achieved during exercise

* `output` : the doctor's diagnosis of whether the patient is at risk for a heart attack
    + 0 = not at risk of heart attack 
    + 1 = at risk of heart attack

Although it is not a formal question on this assignment, you should begin by reading in the dataset and briefly exploring and summarizing the data, and by adjusting any variables that need cleaning.

```{r}
ha <- read_csv("https://www.dropbox.com/s/aohbr6yb9ifmc8w/heart_attack.csv?dl=1")
head(ha)
```


```{r}
ha <- read_csv("https://www.dropbox.com/s/aohbr6yb9ifmc8w/heart_attack.csv?dl=1")
```

```{r}
ha <- ha %>%
  mutate(
    sex = factor(sex),
    cp = factor(cp),
    restecg = factor(restecg),
    output = factor(output, levels = c(1,0)) # make "at-risk" the primary target
  ) %>%
  drop_na()
```


# Part One: Fitting Models

This section asks you to create a final best model for each of the model types studied this week.  For each, you should:

* Find the best model based on `roc.auc` for predicting the `target` variable.

* Output a **confusion matrix**; that is, the counts of how many observations fell into each predicted class for each true class.  (Hint: Code is provided from lecture; alternatively, `conf_mat` is a nice shortcut function for this task.)

* Report the (cross-validated!) `roc.auc` metric.

* Fit the final model.

* (Where applicable) Interpret the coefficients and/or estimates produced by the model fit.

You should certainly try multiple model recipes to find the best model. However, you do not need to include these attempts in your writeup.  You *should* include any hyperparameter tuning steps in your writeup, though.

#### Q1: KNN

#### Q2: Logistic Regression

#### Q3: LDA

#### Q4: QDA

#### Q5: Interpretation

Which predictors were most important to predicting heart attack risk?

#### Q6:  ROC Curve

Plot the ROC Curve for your favorite model from Q1-4.

```{r}
ha %>%
  mutate(
    preds = predict(model_final, ha, type = "prob")$.pred_1
  ) %>%
  roc_curve(
    truth = output,
    preds
  ) %>%
  autoplot()
```


# Part Two: Metrics

Consider the following metrics:

* **True Positive Rate** or **Recall** or **Sensitivity** = Of the observations that are truly Class A, how many were predicted to be Class A?

* **Precision** or **Positive Predictive Value** = Of all the observations classified as Class A, how many of them were truly from Class A?

* **True Negative Rate** or **Specificity** or **Negative Predictive Value** = Of all the observations classified as NOT Class A, how many were truly NOT Class A?

Compute each of these metrics (cross-validated) for your four models in Part One.

Hint: You can specify which metrics you want to calculate as follows:

```{r, eval = FALSE}
my_workflow %>%
  fit_resamples(cvs,
                metrics = metric_set(accuracy, roc_auc)) %>%
  collect_metrics()
```


# Part Three:  Discussion

Suppose you have been hired by a hospital to create classification models for heart attack risk.

The following questions give a possible scenario for why the hospital is interested in these models.  For each one, discuss:

* Which metric(s) you would use for model selection and why.

* Which of your final models (Part One Q1-4) you would recommend to the hospital, and why.

* What score you should expect for your chosen metric(s) using your chosen model to predict future observations.

#### Q1

The hospital faces severe lawsuits if they deem a patient to be low risk, and that patient later experiences a heart attack.

#### Q2

The hospital is overfull, and wants to only use bed space for patients most in need of monitoring due to heart attack risk.

#### Q3

The hospital is studying root causes of heart attacks, and would like to understand which biological measures are associated with heart attack risk.

#### Q4

The hospital is training a new batch of doctors, and they would like to compare the diagnoses of these doctors to the predictions given by the algorithm to measure the ability of new doctors to diagnose patients.

# Part Four: Validation

Before sharing the dataset with you, I set aside a random 10% of the observations to serve as a final validation set.

```{r}
ha_validation <- read_csv("https://www.dropbox.com/s/jkwqdiyx6o6oad0/heart_attack_validation.csv?dl=1")
```


```{r}
ha_validation <- ha_validation %>%
  mutate(
    sex = factor(sex),
    cp = factor(cp),
    restecg = factor(restecg),
    output = factor(output, levels = c(1,0)) # make "at-risk" the primary target
  ) %>%
  drop_na()
```


Use each of your final models in Part One Q1-4, predict the `output` variable in the validation dataset.

For each, output a confusion matrix, and report the `roc.auc`, the `precision`, and the `recall`.

Compare these values to the cross-validated estimates you reported in Part One and Part Two.  Did our measure of model success turn out to be approximately correct for the validation data?

# Challenges:  

## Challenge 1: Cohen's Kappa

Another common metric used in classification is Cohen's Kappa.

For **5 Challenge Points**, use online resources to research this measurement. Calculate it for the models from Part One, Q1-4, and discuss reasons or scenarios that would make us prefer to use this metric as our measure of model success.

## Challenge 2:  Multiple categories

Suppose that instead of predicting the heart attack risk, we were trying to predict the type of chest pain experienced by an individual (`cp`).

For **5 Challenge Points**: 

Try fitting each of the four types of models to predict `cp`.  Which ones work, and which ones do not?  Give an intuition for why this is true.  

For **5 more Challenge Points**: 

For the models that worked, which metrics make sense to measure model success, and which do not?  Why?

